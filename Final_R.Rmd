---
title: "Bayesian Research Project"
author: "Kevin Choi"
date: "`r Sys.Date()`"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-pkg, include=FALSE, message = FALSE}
library(ggplot2)  # for plots
library(magrittr)  # for `%>%` operator
library(here)
library(readxl)  # for reading excel files
library(modelsummary)  # for summarizing data
library(rstan)
rstan_options(auto_write = TRUE)  # save compiled STAN object
options(mc.cores = 2)  # use two cores
library(posterior)
library(bayesplot)
library(psych)
library(tidyverse)
library(dplyr)
library(haven)
library(corrplot)
library(blavaan)
library(semPlot)


theme_set(theme_classic() +
    theme(panel.grid.major.y = element_line(color = "grey92")))
```

# Research Question

> Is there a difference in how people imagine the future self when they are primed to think about the self as stable or changeable?

# Variables

- `Vividness`: subjective rating of the vividness of the imagined future self (2 items on a 6 point scale averaged)
- `Connectedness`: subjective rating of the perceived connectedness of the imagined future self (2 items on a 6 point scale averaged)
- `Distance`: subjective rating of the temporal distance of the imagined future self (2 items on a 6 point scale averaged)
- `Relevance`: subjective rating of how relevant the imagined future self feels (2 items on a 6 point scale averaged)
- `Condition`: 1 = self-stability, 2 = self-change

## Import Data

```{r data}
data <- read_sav("data.sav")
```

## Variable Summary

```{r summ-var}

#Data
# describe(data)

##Summary Statistics for Variables of Interest 
describeBy(data.frame(data$VIVD, data$CONNECT, data$DIST, data$RELEVANCE), data$COND)

##Correlation Matrix 
source("http://www.sthda.com/upload/rquery_cormat.r")
#Combined 
rquery.cormat(data[c("VIVD","CONNECT","DIST","RELEVANCE")], type="flatten")

#By Condition
data.short <- data[c("COND","VIVD","CONNECT","DIST","RELEVANCE")]


#Exploratory Factor Analysis



```

# Model

Let $Y$ = VIVIDNESS, CONNECTION, TEMPORAL DISTNACE, & RELEVANCE, $G$ = CONDITION (CONTINUITY v CHANGE)

Model:
$$
  \begin{aligned}
    Y_{i, G = 0} & \sim N(\mu_1, \sigma_1) \\
    Y_{i, G = 1} & \sim N(\mu_2, \sigma_2)
  \end{aligned}
$$

Prior:
$$
  \begin{aligned}
    \mu_1 & \sim N(0, 1) \\
    \mu_2 & \sim N(0, 1) \\
    \sigma_1 & \sim N^+(0, 1) \\
    \sigma_2 & \sim N^+(0, 1)
  \end{aligned}
$$
While non-informative priors are attractive in the sense of minimizing prior inputs, they also ensure that the Bayes factor depends on the data only through the two-sample t statistic

## Running Stan

We used 4 chains, each with 4,000 iterations (first 2,000 as warm-ups). 

```{r stan, results = "hide"}
# 1. form the data list for Stan
vivid <- with(data,
    list(N1 = sum(COND == 1),
         N2 = sum(COND == 2),
         y1 = VIVD[which(COND == 1)],
         y2 = VIVD[which(COND == 2)])
)

conn <- with(data,
    list(N1 = sum(COND == 1),
         N2 = sum(COND == 2),
         y1 = CONNECT[which(COND == 1)],
         y2 = CONNECT[which(COND == 2)])
)

dist <- with(data,
    list(N1 = sum(COND == 1),
         N2 = sum(COND == 2),
         y1 = DIST[which(COND == 1)],
         y2 = DIST[which(COND == 2)])
)

rel <- with(data,
    list(N1 = sum(COND == 1),
         N2 = sum(COND == 2),
         y1 = RELEVANCE[which(COND == 1)],
         y2 = RELEVANCE[which(COND == 2)])
)

dys_plan <- with(data,
    list(N1 = sum(COND == 1),
         N2 = sum(COND == 2),
         y1 = PLAN[which(COND == 1)],
         y2 = PLAN[which(COND == 2)])
)

time_prep <- with(data,
    list(N1 = sum(COND == 1),
         N2 = sum(COND == 2),
         y1 = PREP[which(COND == 1)],
         y2 = PREP[which(COND == 2)])
)
    
    

# 2. Run Stan 
m.vivid <- stan(
    file = "stan.stan",
    data = vivid,
    seed = 1234,  # for reproducibility
    iter = 4000
)

m.conn <- stan(
    file = "stan.stan",
    data = conn,
    seed = 1234,  # for reproducibility
    iter = 4000
)

m.dist <- stan(
    file = "stan.stan",
    data = dist,
    seed = 1234,  # for reproducibility
    iter = 4000
)

m.rel <- stan(
    file = "stan.stan",
    data = rel,
    seed = 1234,  # for reproducibility
    iter = 4000
)

m.plan <- stan(
    file = "stan.stan",
    data = dys_plan,
    seed = 1234,  # for reproducibility
    iter = 4000
)

m.prep <- stan(
    file = "stan.stan",
    data = time_prep,
    seed = 1234,  # for reproducibility
    iter = 4000
)


```

# Results

As shown in the graph below, the chains mixed well and the ESS is very high. 

```{r rank-hist-vivid}
# print(m.vivid, pars = c("mu1", "mu2", "sigma1", "sigma2"))

m.vivid %>%
    as_draws() %>%
    subset_draws(variable = c("mu1", "mu2", "sigma1", "sigma2")) %>%
    summarize_draws() %>%
    knitr::kable()

mcmc_rank_hist(m.vivid, pars = c("mu1", "mu2", "sigma1", "sigma2"))

```

```{r rank-hist-conn}
m.conn %>%
    as_draws() %>%
    subset_draws(variable = c("mu1", "mu2", "sigma1", "sigma2")) %>%
    summarize_draws() %>%
    knitr::kable()

mcmc_rank_hist(m.conn, pars = c("mu1", "mu2", "sigma1", "sigma2"))

```

```{r rank-hist-dist}
m.dist %>%
    as_draws() %>%
    subset_draws(variable = c("mu1", "mu2", "sigma1", "sigma2")) %>%
    summarize_draws() %>%
    knitr::kable()

mcmc_rank_hist(m.dist, pars = c("mu1", "mu2", "sigma1", "sigma2"))

```

```{r rank-hist-rel}
m.rel %>%
    as_draws() %>%
    subset_draws(variable = c("mu1", "mu2", "sigma1", "sigma2")) %>%
    summarize_draws() %>%
    knitr::kable()

mcmc_rank_hist(m.rel, pars = c("mu1", "mu2", "sigma1", "sigma2"))

```

The following table shows the posterior distributions of $\mu_1$, $\mu_2$, $\sigma_1$, $\sigma_2$, and $\mu_2 - \mu_1$.

```{r summ-vivid}
summ_m.vivid <- as_draws_df(m.vivid) %>%
    subset_draws(variable = c("mu1", "mu2", "sigma1", "sigma2")) %>%
    mutate_variables(`mu2 - mu1` = mu2 - mu1) %>%
    summarise_draws()
knitr::kable(summ_m.vivid, digits = 2)
```

```{r summ-conn}
summ_m.conn <- as_draws_df(m.conn) %>%
    subset_draws(variable = c("mu1", "mu2", "sigma1", "sigma2")) %>%
    mutate_variables(`mu2 - mu1` = mu2 - mu1) %>%
    summarise_draws()
knitr::kable(summ_m.conn, digits = 2)
```

```{r summ-dist}
summ_m.dist <- as_draws_df(m.dist) %>%
    subset_draws(variable = c("mu1", "mu2", "sigma1", "sigma2")) %>%
    mutate_variables(`mu2 - mu1` = mu2 - mu1) %>%
    summarise_draws()
knitr::kable(summ_m.dist, digits = 2)
```

```{r summ-rel}
summ_m.rel <- as_draws_df(m.rel) %>%
    subset_draws(variable = c("mu1", "mu2", "sigma1", "sigma2")) %>%
    mutate_variables(`mu2 - mu1` = mu2 - mu1) %>%
    summarise_draws()
knitr::kable(summ_m.rel, digits = 2)
```
The analysis showed that on average, the two conditions did not differ significantly in how vividly they imagined the future self, with a posterior mean of `r round(summ_m.vivid$mean[5], 2)`. However, the stability condition did perceive the future self as slightly more connected `r round(summ_m.conn$mean[5], 2)`, temporally closer `r round(summ_m.dist$mean[5], 2)`, and more relevant `r round(summ_m.rel$mean[5], 2)`.

#Exploratory Analysis

```{r rank-hist-planprep}

m.plan %>%
    as_draws() %>%
    subset_draws(variable = c("mu1", "mu2", "sigma1", "sigma2")) %>%
    summarize_draws() %>%
    knitr::kable()

mcmc_rank_hist(m.plan, pars = c("mu1", "mu2", "sigma1", "sigma2"))

m.prep %>%
    as_draws() %>%
    subset_draws(variable = c("mu1", "mu2", "sigma1", "sigma2")) %>%
    summarize_draws() %>%
    knitr::kable()

mcmc_rank_hist(m.prep, pars = c("mu1", "mu2", "sigma1", "sigma2"))

```


``` {r summ-planprep}
summ_m.plan <- as_draws_df(m.plan) %>%
    subset_draws(variable = c("mu1", "mu2", "sigma1", "sigma2")) %>%
    mutate_variables(`mu2 - mu1` = mu2 - mu1) %>%
    summarise_draws()
knitr::kable(summ_m.plan, digits = 2)

summ_m.prep <- as_draws_df(m.prep) %>%
    subset_draws(variable = c("mu1", "mu2", "sigma1", "sigma2")) %>%
    mutate_variables(`mu2 - mu1` = mu2 - mu1) %>%
    summarise_draws()
knitr::kable(summ_m.prep, digits = 2)

```

``` {r corr}
rquery.cormat(data[c("VIVD","CONNECT","DIST","RELEVANCE", "PLAN", "PREP")], type="flatten")


```

``` {r bayesian latent variable model}
model.full1 = ' Vivid =~ CG_1 + CGR_2
                Connect =~ CG_3 + CG_4R
                Distance =~ CG_5 + CG_6R
                Relevance =~ CG_7R + CG_8

Vivid ~ COND
Connect ~ COND
Distance ~ COND
Relevance ~ COND 
'

model.full1.fit <- blavaan(model.full1, data = data, auto.var=TRUE, auto.fix.first=TRUE, auto.cov.lv.x=TRUE)
summary(model.full1.fit, fit.measures=TRUE, standardized=TRUE) 
coef(model.full1.fit)
ML <- blavFitIndices(model.full1.fit)
summary(ML)


semPaths(model.full1.fit, what="est", edge.label.cex = .9, fade = FALSE,intercepts = FALSE, residuals = FALSE, layoutSplit = T, layout = 'tree2', nCharNodes = 0, sizeMan = 7 )

```
